{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cb5307a",
   "metadata": {},
   "source": [
    "### SparkContext - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac6faf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext()\n",
    "print(sc)\n",
    "print(type(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b12d2b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체가 잘 만들어졌음을 볼 수 있습니다. 만약 SparkContext를 한 개 더 만들면 어떻게 될까요?\n",
    "# 에러 발생\n",
    "\n",
    "# new_sc = SparkContext()\n",
    "# ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/5p/l3b0fr1x5wn7450vgnp88r080000gn/T/ipykernel_1755/3605226842.py:4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58083420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext 종료\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34685f3f",
   "metadata": {},
   "source": [
    "### SparkContext - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f3e702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=pyspark test>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(master='local', appName='pyspark test')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02665c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.startTime', '1664642777215'),\n",
       " ('spark.driver.host', '172.16.227.180'),\n",
       " ('spark.app.id', 'local-1664642777295'),\n",
       " ('spark.app.name', 'pyspark test'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '56178')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparkContext의 Configuration을 확인하기 위해서 .getConf().getAll()을 이용합니다.\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9bb681a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark test\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "print(sc.appName)\n",
    "print(sc.master)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ac5c98",
   "metadata": {},
   "source": [
    "### SparkContext - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309c30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=Pyspark Test1>\n",
      "Pyspark Test1\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "# SparkConf()을 이용해 SparkContext의 Configuration을 설정하는 방법으로 SparkContext를 만들 수 있습니다\n",
    "# .setMaster(), setAppName()을 이용해 어플리케이션의 이름과 Master의 URL을 설정해줄 수 있습니다.\n",
    "\n",
    "conf = SparkConf().setAppName(\"Pyspark Test1\").setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc)\n",
    "print(sc.appName)\n",
    "print(sc.master)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b2a45",
   "metadata": {},
   "source": [
    "### SparkContext - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678366be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Example App\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"Spark Example App\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "print(sc.appName)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a608d4",
   "metadata": {},
   "source": [
    "### SparkContext - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cadfade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=Pyspark Test1>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setAppName(\"Pyspark Test1\").setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466b5129",
   "metadata": {},
   "source": [
    "### SparkSession - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa3d19ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.227.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark-2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x113413c90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pyspark-2\").master('local[*]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1be4b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Pyspark-2>\n",
      "Pyspark-2\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext)\n",
    "print(spark.sparkContext.appName)\n",
    "\n",
    "# SparkContext stop() method\n",
    "# spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4762fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create RDD\n",
    "rdd = spark.sparkContext.range(1, 5)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d00ad48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1664642816099\n",
      "http://172.16.227.180:4040\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "print(spark.sparkContext.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c768fed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello world!']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "tempdir = '/Users/sunghwanki/Desktop/Project/Python_Advance/Pyspark'\n",
    "path = os.path.join(tempdir, 'sample.txt')\n",
    "\n",
    "with open(path, \"w\") as testFile:\n",
    "    testFile.write(\"Hello world!\")\n",
    "\n",
    "rdd = spark.sparkContext.textFile(path)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33053b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macOS 12.5                       March 13, 2021                       macOS 12.5',\n",
       " 'S\\x08SY\\x08YN\\x08NO\\x08OP\\x08PS\\x08SI\\x08IS\\x08S',\n",
       " 'S\\x08ST\\x08TA\\x08AN\\x08ND\\x08DA\\x08AR\\x08RD\\x08DS\\x08S',\n",
       " 'S\\x08SE\\x08EE\\x08E A\\x08AL\\x08LS\\x08SO\\x08O',\n",
       " 'P\\x08PR\\x08RI\\x08IM\\x08MA\\x08AR\\x08RI\\x08IE\\x08ES\\x08S',\n",
       " 'O\\x08OP\\x08PE\\x08ER\\x08RA\\x08AT\\x08TO\\x08OR\\x08RS\\x08S',\n",
       " 'N\\x08NA\\x08AM\\x08ME\\x08E',\n",
       " 'H\\x08HI\\x08IS\\x08ST\\x08TO\\x08OR\\x08RY\\x08Y',\n",
       " 'FIND(1)                      General Commands Manual                     FIND(1)',\n",
       " 'E\\x08EX\\x08XA\\x08AM\\x08MP\\x08PL\\x08LE\\x08ES\\x08S']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.textFile('./test.txt')\n",
    "rdd.top(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c9ac49e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1, 2, 3]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "myList = [1,2,3,4,5]\n",
    "myRdd = spark.sparkContext.parallelize(myList)\n",
    "print(myRdd.first())\n",
    "print(myRdd.take(3))\n",
    "print(myRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d79a00d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./test2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./test2.txt\n",
    "my name is danny\n",
    "your name is john\n",
    "we are friend\n",
    "good morning\n",
    "the king\n",
    "may the force be with you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5fd57101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is danny\n",
      "['my name is danny', 'your name is john', 'we are friend']\n",
      "['my name is danny', 'your name is john', 'we are friend', 'good morning', 'the king', 'may the force be with you']\n"
     ]
    }
   ],
   "source": [
    "myRdd1 = spark.sparkContext.textFile(os.path.join(tempdir, 'test2.txt'))\n",
    "print(myRdd1.first())\n",
    "print(myRdd1.take(3))\n",
    "print(myRdd1.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24b1ef0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dataRange = list(range(1,10))\n",
    "print(dataRange)\n",
    "print(type(dataRange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ea81834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[47] at readRDDFromFile at PythonRDD.scala:274\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "rangeRdd = spark.sparkContext.parallelize(dataRange)\n",
    "print(rangeRdd)\n",
    "print(type(rangeRdd))\n",
    "print(rangeRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fdc00707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "def sub(n):\n",
    "    return n-1\n",
    "\n",
    "subRdd = rangeRdd.map(sub)\n",
    "print(subRdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13f556",
   "metadata": {},
   "source": [
    "flatMap()\n",
    "\n",
    "map과 비슷하지만 map에서 이중 리스트로 표현되는 데이터를 하나의 리스트에서 전부 나열해줍니다.\n",
    "\n",
    "아래 4개의 문자열이 있는 리스트를 RDD로 생성하고 동일한 lambda 함수를 인자로 각각 map과 flatMap을 사용합니다.\n",
    "\n",
    "map을 사용한 RDD는 lambda 함수로 인해 문자열과 문자열 끝에 s가 붙은 문자열의 쌍으로 이루어져 있지만 flatMap을 사용한 RDD는 모든 문자열이 하나의 리스트안에 나열되어있습니다.\n",
    "\n",
    "각 RDD를 count함수를 사용하면 결과가 다르게 나오는 것을 확인할 수 있습니다.\n",
    "\n",
    "이렇게 모든 데이터를 하나의 리스트로 변환해주기 때문에 워드카운트와 같은 작업에 flatMap이 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09a285cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'elephant', 'tiger']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = ['cat','dog','elephant', 'tiger']\n",
    "animalsRdd = spark.sparkContext.parallelize(animals)\n",
    "animalsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71ead82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'cats'), ('dog', 'dogs'), ('elephant', 'elephants'), ('tiger', 'tigers')]\n",
      "['cat', 'cats', 'dog', 'dogs', 'elephant', 'elephants', 'tiger', 'tigers']\n"
     ]
    }
   ],
   "source": [
    "animalsRDDMap = animalsRdd.map(lambda x: (x, x+'s'))\n",
    "animalsRDDFlatmap = animalsRdd.flatMap(lambda x: (x,x+'s'))\n",
    "\n",
    "print(animalsRDDMap.collect())\n",
    "print(animalsRDDFlatmap.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6f26cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(animalsRDDMap.count())\n",
    "print(animalsRDDFlatmap.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5795a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56995fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interview', 'interviewbit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Transdormation Demo\")\n",
    "words_list = sc.parallelize (\n",
    "  [\"pyspark\", \n",
    "  \"interview\", \n",
    "  \"questions\", \n",
    "  \"at\", \n",
    "  \"interviewbit\"]\n",
    ")\n",
    "filtered_words = words_list.filter(lambda x: 'interview' in x)\n",
    "filtered = filtered_words.collect()\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da0f444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of elements in RDD ->  2\n"
     ]
    }
   ],
   "source": [
    "counts = filtered_words.count()\n",
    "print(\"Count of elements in RDD -> \",  counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce15199",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28316b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd6683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59222d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9768526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title : PySpark Script Template\n",
    "# Description : This template can be used to create pyspark script\n",
    "# Author : sqlandhadoop.com\n",
    "# Date : 30-June-2021\n",
    "# Version : 1.0 (Initial Draft)\n",
    "# Usage : spark-submit --executor-memory 4G --executor-cores 4 PySpark_Script_Template.py > ./PySpark_Script_Template.log 2>&1 &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d47c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import sys, logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2511a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging configuration\n",
    "formatter = logging.Formatter('[%(asctime)s] %(levelname)s @ line % (lineno)d: %(message)s')\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevle(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90deafeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import sys,logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Logging configuration\n",
    "formatter = logging.Formatter('[%(asctime)s] %(levelname)s @ line %(lineno)d: %(message)s')\n",
    "handler = logging.StreamHandler(sys.stdout)\n",
    "handler.setLevel(logging.INFO)\n",
    "handler.setFormatter(formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# current time variable to be used for logging purpose\n",
    "dt_string = datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "# change it to your app name\n",
    "AppName = \"MyPySparkApp\"\n",
    "\n",
    "\n",
    "# adding dummy function. change or remove it.\n",
    "def some_function1():\n",
    "    logger.info(\"Inside some_function 1\")\n",
    "\n",
    "# adding dummy function. change or remove it.\n",
    "def some_function2():\n",
    "    logger.info(\"Inside some_function 2\")\n",
    "\n",
    "def main():\n",
    "    # start spark code\n",
    "    spark = SparkSession.builder.appName(AppName+\"_\"+str(dt_string)).getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    logger.info(\"Starting spark application\")\n",
    "\n",
    "    #calling function 1\n",
    "    some_function1()\n",
    "\n",
    "    #calling function 2\n",
    "    some_function2()\n",
    "\n",
    "    #do something here\n",
    "    logger.info(\"Reading CSV File\")\n",
    "    df_category = spark.read.option(\"delimiter\",\"|\").csv(\"hdfs:///var/data/category_pipe.txt\")\n",
    "    logger.info(\"Previewing CSV File Data\")\n",
    "    df_category.show(truncate=False)\n",
    "\n",
    "    logger.info(\"Ending spark application\")\n",
    "    # end spark code\n",
    "    spark.stop()\n",
    "    return None\n",
    "\n",
    "# Starting point for PySpark\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    sys.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
