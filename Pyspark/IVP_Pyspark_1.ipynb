{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af51b82",
   "metadata": {},
   "source": [
    "source : https://www.interviewbit.com/pyspark-interview-questions/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a4a4f",
   "metadata": {},
   "source": [
    "## What is PySpark?\n",
    "\n",
    "PySpark is an Apache Spark interface in Python. \n",
    "It is used for collaborating with Spark using APIs written in Python. \n",
    "It also supports Spark’s features like Spark DataFrame, Spark SQL, Spark Streaming, Spark MLlib and Spark Core. \n",
    "It provides an interactive PySpark shell to analyze structured and semi-structured data in a distributed environment.\n",
    "PySpark supports reading data from multiple sources and different formats. It also facilitates the use of RDDs (Resilient Distributed Datasets).\n",
    "\n",
    "PySpark can be installed by using the pip command:\n",
    "\n",
    "```\n",
    "pip install pyspark\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864a25d",
   "metadata": {},
   "source": [
    "## What are the advantages and disadvantages of PySpark?\n",
    "### Advantages of PySpark:\n",
    "- Library Support: Compared to Scala, Python has a huge library collection for working in the field of data science.\n",
    "- Easy to Learn: PySpark is an easy to learn language.\n",
    "\n",
    "### Disadvantages of PySpark:\n",
    "- Since Spark was originally developed in Scala, while using PySpark in Python programs they are relatively less efficient and approximately 10x times slower than the Scala programs. This would impact the performance of heavy data processing applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93730d52",
   "metadata": {},
   "source": [
    "## SparkSession vs SparkContext\n",
    "\n",
    "SparkContext is an entry point to Spark programming with RDD and to connect to the Spark Cluster, Since Spark 2.0 SparkSession has been introduced and became an entry point to start programming with DataFrame and Dataset.\n",
    "\n",
    "### SparkContext \n",
    "Spark SparkContext is an entry point to Spark and since 1.x and used to create Spark RDD and broadcast variables on the cluster. \n",
    "\n",
    "Since Spark 2.0 most of the functionalities (methods) available in SparkContext are also available in SparkSession. Its object **sc** is default available in spark-shell and it can be created by using SparkContext class\n",
    "\n",
    "Note that you can create only one SparkContext per JVM, in order to create another first you need to stop the existing one using stop() method.\n",
    "\n",
    "The Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs, and know what resource manager (YARN, Mesos, or Standalone) to communicate to. It is the heart of the PySpark application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d2195",
   "metadata": {},
   "source": [
    "<img width=\"608\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193417217-225c59d5-e1ab-43b3-b972-99e547669ed5.png\">\n",
    "\n",
    "PySpark에서 선언하는 SparkContext 객체는 내부의 JVM(Java Virtual Machine) 위에 동작하는 Py4J의 SparkContext와 연결됩니다.\n",
    "Py4J의 SparkContext는 Worker 노드들과도 연결되어 있고, Worker 노드들 역시 실제 동작은 JVM 위에서 동작합니다.\n",
    "정리하자면, PySpark는 Python으로 코딩을 하긴 하지만 실제 동작은 JVM에서 행해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340fa08",
   "metadata": {},
   "source": [
    "<img width=\"741\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193417776-a3cf025d-847d-4b00-a7ba-9343beba2366.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede61565",
   "metadata": {},
   "source": [
    "### SparkContext - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a61494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=pyspark-shell>\n",
      "<class 'pyspark.context.SparkContext'>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc = SparkContext()\n",
    "print(sc)\n",
    "print(type(sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac16f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체가 잘 만들어졌음을 볼 수 있습니다. 만약 SparkContext를 한 개 더 만들면 어떻게 될까요?\n",
    "# 에러 발생\n",
    "\n",
    "# new_sc = SparkContext()\n",
    "# ValueError: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at /var/folders/5p/l3b0fr1x5wn7450vgnp88r080000gn/T/ipykernel_1755/3605226842.py:4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3f280ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkContext 종료\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cdfefb",
   "metadata": {},
   "source": [
    "### SparkContext - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cfaf85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=pyspark test>\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(master='local', appName='pyspark test')\n",
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f86265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1665282114640'),\n",
       " ('spark.app.name', 'pyspark test'),\n",
       " ('spark.driver.host', '192.168.1.8'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.startTime', '1665282114586'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '49419')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sparkContext의 Configuration을 확인하기 위해서 .getConf().getAll()을 이용합니다.\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bed9b2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark test\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "print(sc.appName)\n",
    "print(sc.master)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f9f1d",
   "metadata": {},
   "source": [
    "### SparkContext - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "862a9eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local appName=Pyspark Test1>\n",
      "Pyspark Test1\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "# SparkConf()을 이용해 SparkContext의 Configuration을 설정하는 방법으로 SparkContext를 만들 수 있습니다\n",
    "# .setMaster(), setAppName()을 이용해 어플리케이션의 이름과 Master의 URL을 설정해줄 수 있습니다.\n",
    "\n",
    "conf = SparkConf().setAppName(\"Pyspark Test1\").setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "print(sc)\n",
    "print(sc.appName)\n",
    "print(sc.master)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbf7bc5",
   "metadata": {},
   "source": [
    "### SparkContext - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb1023a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pyspark Test2\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "# Create Spark Context\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local\").setAppName(\"Pyspark Test2\")\n",
    "\n",
    "#sc = SparkContext(conf=conf)\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "print(sc.appName)\n",
    "print(sc.master)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6e153",
   "metadata": {},
   "source": [
    "### SparkSession\n",
    "SparkSession introduced in version 2.0 and is an entry point to Spark functionality in order to create Spark RDD, DataFrame and DataSet. Its object **spark** is default available in spark-shell and it can be created using SparkSession class\n",
    "\n",
    "SparkSession is a Unified API that is used in replacing the SQLContext, StreamingContext, HiveContext and all other contexts.\n",
    "\n",
    "- Spark Context,\n",
    "- SQL Context,\n",
    "- Streaming Context,\n",
    "- Hive Context.\n",
    "\n",
    "\n",
    "SparkSession is the replacement of SparkContext since PySpark version 2.0. This acts as a starting point to access all of the PySpark functionalities related to RDDs, DataFrame, Datasets etc. \n",
    "\n",
    "The SparkSession internally creates SparkContext and SparkConfig based on the details provided in SparkSession. SparkSession can be created by making use of builder patterns.\n",
    "\n",
    "<img width=\"778\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193422833-b9567d2d-19ce-4bca-8224-e23c7e65c51b.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea021deb",
   "metadata": {},
   "source": [
    "You can stop the SparkContext by calling the stop() method. As explained above you can have only one SparkContext per JVM. If you wanted to create another, you need to shutdown it first by using stop() method and create a new SparkContext.\n",
    "\n",
    "```aidl\n",
    "# SparkContext stop() method\n",
    "spark.sparkContext.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cff4d",
   "metadata": {},
   "source": [
    "### SparkSession - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48cc1505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.16.227.180:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Pyspark Test3</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x111649d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pyspark Test3\").master('local[*]').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c83e5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=Pyspark Test3>\n",
      "Pyspark Test3\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext)\n",
    "print(spark.sparkContext.appName)\n",
    "\n",
    "# SparkContext stop() method\n",
    "# spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d4473",
   "metadata": {},
   "source": [
    "### SparkContext 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2cae31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0997489d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create RDD\n",
    "rdd = sc.range(1,5)\n",
    "# rdd = spark.sparkContext.range(1, 5)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9fc21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1665283057965\n",
      "http://192.168.1.8:4040\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(sc.applicationId)\n",
    "print(sc.uiWebUrl)\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "288c5891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1665574094899\n",
      "http://172.16.227.180:4040\n",
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "print(spark.sparkContext.applicationId)\n",
    "print(spark.sparkContext.uiWebUrl)\n",
    "print(spark.sparkContext.version)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f62c9",
   "metadata": {},
   "source": [
    "## RDD\n",
    "RDD는 Resilient Distributed Dataset의 약자이다.\n",
    "Resilient는 작업이 실패하지 않도록 falut tolerent 한 것이며 어느 한 노드에서 작업이 실패하면 다른 노드에서 실행하는 것을 의미한다.\n",
    "Rdd는 수정할 수 없는 Read-Only 이다.\n",
    "\n",
    "RDDs stand for Resilient Distributed Datasets. These are the elements that are used for running and operating on multiple nodes to perform parallel processing on a cluster. Since RDDs are suited for parallel processing, they are immutable elements. This means that once we create RDD, we cannot modify it. RDDs are also fault-tolerant which means that whenever failure happens, they can be recovered automatically. Multiple operations can be performed on RDDs to perform a certain task. The operations can be of 2 types:\n",
    "\n",
    "<img width=\"781\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193422414-a2d9b075-028b-425f-836b-b2e9164cc493.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24ae78",
   "metadata": {},
   "source": [
    "### Transformation: \n",
    "These operations when applied on RDDs result in the creation of a new RDD. Some of the examples of transformation operations are filter, groupBy, map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1a5f336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interview', 'interviewbit']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Pyspark Test4\")\n",
    "words_list = sc.parallelize (\n",
    "  [\"pyspark\", \n",
    "  \"interview\", \n",
    "  \"questions\", \n",
    "  \"at\", \n",
    "  \"interviewbit\"]\n",
    ")\n",
    "filtered_words = words_list.filter(lambda x: 'interview' in x)\n",
    "filtered = filtered_words.collect()\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebebda",
   "metadata": {},
   "source": [
    "<img width=\"1677\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/194735235-5796d8f9-b1c9-4d08-871b-9fece2a92ef0.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61f5083",
   "metadata": {},
   "source": [
    "### Action :\n",
    "These operations instruct Spark to perform some computations on the RDD and return the result to the driver. It sends data from the Executer to the driver. count(), collect(), take() are some of the examples.\n",
    "Let us consider an example to demonstrate action operation by making use of the count() function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e0b885",
   "metadata": {},
   "source": [
    "- path 모듈 사용 : https://yeo0.github.io/pg/2018/11/21/%ED%8C%8C%EC%9D%B4%EC%8D%AC-os.path-%EB%AA%A8%EB%93%88/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f52688c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['Hello my world', 'danny ']\n",
      "['Hello my world', 'danny ', 'good morning', 'I am good', 'how are you?', 'good ', 'bad', 'my']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "temp = '/Users/sunghwanki/Desktop/Project/Python_Advance/Pyspark'\n",
    "\n",
    "# path = os.path.join(tempdir, 'sample.txt')\n",
    "path = os.path.abspath('./sample.txt')\n",
    "\n",
    "with open(path, \"w\") as f:\n",
    "    f.write('''Hello my world\n",
    "danny \n",
    "good morning\n",
    "I am good\n",
    "how are you?\n",
    "good \n",
    "bad\n",
    "my''')\n",
    "\n",
    "rdd = sc.textFile(path)\n",
    "print(rdd.count())\n",
    "print(rdd.take(2))\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a06dc87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macOS 12.6                       March 13, 2021                       macOS 12.6',\n",
       " 'S\\x08SY\\x08YN\\x08NO\\x08OP\\x08PS\\x08SI\\x08IS\\x08S',\n",
       " 'S\\x08ST\\x08TA\\x08AN\\x08ND\\x08DA\\x08AR\\x08RD\\x08DS\\x08S',\n",
       " 'S\\x08SE\\x08EE\\x08E A\\x08AL\\x08LS\\x08SO\\x08O',\n",
       " 'P\\x08PR\\x08RI\\x08IM\\x08MA\\x08AR\\x08RI\\x08IE\\x08ES\\x08S']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%man find > test.txt\n",
    "rdd = sc.textFile('./test.txt')\n",
    "rdd.top(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "794e3522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1, 2]\n",
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "mylist = [1,2,3,4,5]\n",
    "rdd = sc.parallelize(mylist)\n",
    "print(rdd.first())\n",
    "print(rdd.take(2))\n",
    "print(rdd.collect())\n",
    "\n",
    "spark.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e7b35",
   "metadata": {},
   "source": [
    "## What are the advantages of PySpark RDD?\n",
    "\n",
    "PySpark RDDs have the following advantages:\n",
    "\n",
    "- In-Memory Processing: PySpark’s RDD helps in loading data from the disk to the memory. The RDDs can even be persisted in the memory for reusing the computations.\n",
    "\n",
    "\n",
    "- Immutability: The RDDs are immutable which means that once created, they cannot be modified. While applying any transformation operations on the RDDs, a new RDD would be created.\n",
    "\n",
    "\n",
    "- Fault Tolerance: The RDDs are fault-tolerant. This means that whenever an operation fails, the data gets automatically reloaded from other available partitions. This results in seamless execution of the PySpark applications.\n",
    "\n",
    "\n",
    "- Lazy Evolution: The transformation are not performed as soon as they are encountered. The operations would be stored in the DAG and are evaluated once it finds the first RDD action.\n",
    "\n",
    "\n",
    "- Partitioning: Whenever RDD is created from any data, the elements in the RDD are partitioned to the cores available by default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b4903a",
   "metadata": {},
   "source": [
    "## What are PySpark serializers?\n",
    "note : https://spark.apache.org/docs/0.9.0/api/pyspark/pyspark.serializers-module.html\n",
    "\n",
    "The serialization process is used to conduct performance tuning on Spark. The data sent or received over the network to the disk or memory should be persisted. PySpark supports serializers for this purpose. It supports two types of serializers, they are:\n",
    "\n",
    "### PickleSerializer: \n",
    "note : https://docs.python.org/2/library/pickle.html\n",
    "\n",
    "This supports almost every Python object.\n",
    "\n",
    "### MarshalSerializer: \n",
    "note : https://docs.python.org/2/library/marshal.html\n",
    "\n",
    "This serializer is faster than the PickleSerializer but it supports only limited types.\n",
    "Consider an example of serialization which makes use of MarshalSerializer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dad66dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 6, 9, 12]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.serializers import MarshalSerializer\n",
    "sc = SparkContext(\"local\", \"Marshal Serialization\", serializer = MarshalSerializer())    #Initialize spark context and serializer\n",
    "print(sc.parallelize(list(range(1000))).map(lambda x: 3 * x).take(5))\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b882457",
   "metadata": {},
   "source": [
    "### pyspark.RDD.glom\n",
    "Return an RDD created by coalescing all elements within each partition into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "15b7b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local', 'test', batchSize=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "834da012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(16), 4)\n",
    "print(rdd.collect())\n",
    "print(rdd.glom().collect())\n",
    "print(rdd._jrdd.count())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3d1ee2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local', 'test', batchSize=1)\n",
    "rdd = sc.parallelize(range(16), 4)\n",
    "print(rdd.collect())\n",
    "print(rdd.glom().collect())\n",
    "print(rdd._jrdd.count())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5d0de94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext('local', 'test', batchSize=2)\n",
    "rdd_without_coalescing = sc.parallelize(range(16))\n",
    "print(rdd_without_coalescing.collect())\n",
    "print(rdd_without_coalescing.glom().collect())\n",
    "print(rdd_without_coalescing._jrdd.count())\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac30e83",
   "metadata": {},
   "source": [
    "## What are the different cluster manager types supported by PySpark?\n",
    "\n",
    "A cluster manager is a cluster mode platform that helps to run Spark by providing all resources to worker nodes based on the requirements.\n",
    "\n",
    "<img width=\"781\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193422610-95ff1441-dd4e-43de-a809-cb93567aeef2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c280999",
   "metadata": {},
   "source": [
    "The above figure shows the position of cluster manager in the Spark ecosystem. Consider a master node and multiple worker nodes present in the cluster. The master nodes provide the worker nodes with the resources like memory, processor allocation etc depending on the nodes requirements with the help of the cluster manager.\n",
    "\n",
    "PySpark supports the following cluster manager types:\n",
    "\n",
    "- Standalone – This is a simple cluster manager that is included with Spark.\n",
    "- Apache Mesos – This manager can run Hadoop MapReduce and PySpark apps.\n",
    "- Hadoop YARN – This manager is used in Hadoop2.\n",
    "- Kubernetes – This is an open-source cluster manager that helps in automated deployment, scaling and automatic management of containerized apps.\n",
    "- local – This is simply a mode for running Spark applications on laptops/desktops."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41018054",
   "metadata": {},
   "source": [
    "## Is PySpark faster than pandas?\n",
    "\n",
    "PySpark supports parallel execution of statements in a distributed environment, i.e on different cores and different machines which are not present in Pandas. This is why PySpark is faster than pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c397a10",
   "metadata": {},
   "source": [
    "## What do you understand about PySpark DataFrames?\n",
    "\n",
    "PySpark DataFrame is a distributed collection of well-organized data that is equivalent to tables of relational databases and is placed into named columns. PySpark DataFrame has better optimisation when compared to R or python. These can be created from different sources like Hive Tables, Structured Data Files, existing RDDs, external databases etc as shown in the image below:\n",
    "\n",
    "<img width=\"776\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193422784-382457d2-f715-4a82-b991-d68c7969d925.png\">\n",
    "\n",
    "The data in the PySpark DataFrame is distributed across different machines in the cluster and the operations performed on this would be run parallelly on all the machines. These can handle a large collection of structured or semi-structured data of a range of petabytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbd0086",
   "metadata": {},
   "source": [
    "## What are the types of PySpark’s shared variables and why are they useful?\n",
    "\n",
    "Whenever PySpark performs the transformation operation using filter(), map() or reduce(), they are run on a remote node that uses the variables shipped with tasks. These variables are not reusable and cannot be shared across different tasks because they are not returned to the Driver. To solve the issue of reusability and sharing, we have shared variables in PySpark. There are two types of shared variables, they are:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2c2b0",
   "metadata": {},
   "source": [
    "### Broadcast variables: \n",
    "These are also known as read-only shared variables and are used in cases of data lookup the requirements. These variables are cached and are made available on all the cluster nodes so that the tasks can make use of them. The variables are not sent with every task. They are rather distributed to the nodes using efficient algorithms for reducing the cost of communication. When we run an RDD job operation that makes use of Broadcast variables, the following things are done by PySpark:\n",
    "\n",
    "- The job is broken into different stages having distributed shuffling. The actions are executed in those stages.\n",
    "- The stages are then broken into tasks.\n",
    "- The broadcast variables are broadcasted to the tasks if the tasks need to use it.\n",
    "\n",
    "\n",
    "Broadcast variables are created in PySpark by making use of the broadcast(variable) method from the SparkContext class. The syntax for this goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e22f03aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 11, 22, 31]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext('local','test')\n",
    "broadcastVar = sc.broadcast([10, 11, 22, 31])\n",
    "broadcastVar.value    # access broadcast variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b868395b",
   "metadata": {},
   "source": [
    "An important point of using broadcast variables is that the variables are not sent to the tasks when the broadcast function is called. They will be sent when the variables are first required by the executors.\n",
    "\n",
    "\n",
    "### Accumulator variables: \n",
    "note : https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.Accumulator.html\n",
    "\n",
    "These variables are called updatable shared variables. They are added through associative and commutative operations and are used for performing counter or sum operations. PySpark supports the creation of numeric type accumulators by default. It also has the ability to add custom accumulator types. The custom types can be of two types:\n",
    "\n",
    "- Named Accumulators: These accumulators are visible under the “Accumulator” tab in the PySpark web UI as shown in the image below:\n",
    "\n",
    "<img width=\"781\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193423081-471f3e8d-dc29-4f7f-928c-a42a70ee3a22.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dafd76",
   "metadata": {},
   "source": [
    "Here, we will see the Accumulable section that has the sum of the Accumulator values of the variables modified by the tasks listed in the Accumulator column present in the Tasks table.\n",
    "\n",
    "- Unnamed Accumulators: These accumulators are not shown on the PySpark Web UI page. It is always recommended to make use of named accumulators.\n",
    "\n",
    "\n",
    "A shared variable that can be accumulated, i.e., has a commutative and associative “add” operation. Worker tasks on a Spark cluster can add values to an Accumulator with the += operator, but only the driver program is allowed to access its value, using value. Updates from the workers get propagated automatically to the driver program.\n",
    "\n",
    "While SparkContext supports accumulators for primitive data types like int and float, users can also define accumulators for custom types by providing a custom AccumulatorParam object. Refer to its doctest for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf6522d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"test\").master('local[*]').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9adedb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "a = sc.accumulator(5)\n",
    "print(a)\n",
    "print(a.value)     # value : Get the accumulator’s value; only usable in driver program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d47388c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "def f(x):\n",
    "    global a\n",
    "    a += x\n",
    "    \n",
    "print(rdd.foreach(f))\n",
    "print(a.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2adac57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "b = sc.accumulator(0)\n",
    "print(b)\n",
    "print(b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "035d0e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "def g(x):\n",
    "    b.add(x)\n",
    "    \n",
    "print(rdd.foreach(g))\n",
    "print(b.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91cf95de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "ac = sc.accumulator(1)\n",
    "print(ac)\n",
    "print(ac.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4edcbf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "ac.value = 3\n",
    "print(ac.value)\n",
    "\n",
    "ac += 5 \n",
    "print(ac.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0bf90cf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3 3\n",
      "4 7\n",
      "7 7\n",
      "8 8\n",
      "9 17\n",
      "6 6\n",
      "0 0\n",
      "2 2\n",
      "1 1\n",
      "5 5\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([i for i in range(10)])\n",
    "\n",
    "def f(x):\n",
    "    global ac\n",
    "    ac += x\n",
    "    print(x, ac)\n",
    "    \n",
    "print(rdd.foreach(f))\n",
    "print(ac.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8eecea",
   "metadata": {},
   "source": [
    "## What is PySpark UDF?\n",
    "\n",
    "UDF stands for User Defined Functions. In PySpark, UDF can be created by creating a python function and wrapping it with PySpark SQL’s udf() method and using it on the DataFrame or SQL. These are generally created when we do not have the functionalities supported in PySpark’s library and we have to use our own logic on the data. UDFs can be reused on any number of SQL expressions or DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8eff115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|Seqno|Name        |\n",
      "+-----+------------+\n",
      "|1    |john jones  |\n",
      "|2    |tracey smith|\n",
      "|3    |amy sanders |\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"Seqno\",\"Name\"]\n",
    "data = [(\"1\", \"john jones\"),\n",
    "    (\"2\", \"tracey smith\"),\n",
    "    (\"3\", \"amy sanders\")]\n",
    "\n",
    "df = spark.createDataFrame(data=data,schema=columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5fbd0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCase(str):\n",
    "    restStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "        restStr = restStr + x[0:1].upper() + x[1:] + \" \"\n",
    "    return restStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55df4303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function <lambda> at 0x11391cdd0>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())\n",
    "print(convertUDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7303e63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+\n",
      "|Seqno|Name         |\n",
      "+-----+-------------+\n",
      "|1    |John Jones   |\n",
      "|2    |Tracey Smith |\n",
      "|3    |Amy Sanders  |\n",
      "+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Seqno\"), \\\n",
    "    convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "382cb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upperCase(str):\n",
    "    return str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "249e4218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+-------------+\n",
      "|Seqno|        Name|Cureated Name|\n",
      "+-----+------------+-------------+\n",
      "|    1|  john jones|   JOHN JONES|\n",
      "|    2|tracey smith| TRACEY SMITH|\n",
      "|    3| amy sanders|  AMY SANDERS|\n",
      "+-----+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "upperCaseUDF = udf(lambda z: upperCase(z))\n",
    "\n",
    "df.withColumn(\"Cureated Name\",upperCaseUDF(col(\"Name\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117770a",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "- Dataset: 구조적 API의 기본 데이터 타입, Scala와 Java에서만 사용 가능\n",
    "- Dataset을 사용할 시기\n",
    "  - 성능 저하를 감수하면서 타입 안정성(type-safe)을 가진 데이터 타입을 사용하고 싶을 때\n",
    "    사용할 Function과 대상 Object의 데이터 타입이 맞지 않는 경우 처럼 데이터 타입이 유효하지 않은 작업을 수행하지 못하도록 방어적 코드, 정확도 높은 프로그램 개발을 위해 Dataset을 사용하는 것이 좋은 선택일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acea755",
   "metadata": {},
   "source": [
    "## What is PySpark Architecture?\n",
    "PySpark similar to Apache Spark works in master-slave architecture pattern. Here, the master node is called the Driver and the slave nodes are called the workers. When a Spark application is run, the Spark Driver creates SparkContext which acts as an entry point to the spark application. All the operations are executed on the worker nodes. The resources required for executing the operations on the worker nodes are managed by the Cluster Managers. The following diagram illustrates the architecture described:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c5705",
   "metadata": {},
   "source": [
    "## What PySpark DAGScheduler?\n",
    "\n",
    "DAG stands for Direct Acyclic Graph. DAGScheduler constitutes the scheduling layer of Spark which implements scheduling of tasks in a stage-oriented manner using jobs and stages. The logical execution plan (Dependencies lineage of transformation actions upon RDDs) is transformed into a physical execution plan consisting of stages. It computes a DAG of stages needed for each job and keeps track of what stages are RDDs are materialized and finds a minimal schedule for running the jobs. These stages are then submitted to TaskScheduler for running the stages. This is represented in the image flow below:\n",
    "\n",
    "<img width=\"776\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/193822492-0028b54f-9f54-43c1-b505-0ec3f6160900.png\">\n",
    "\n",
    "DAGScheduler performs the following three things in Spark:\n",
    "\n",
    "- Compute DAG execution for the job.\n",
    "- Determine preferred locations for running each task\n",
    "- Failure Handling due to output files lost during shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7248d5",
   "metadata": {},
   "source": [
    "## Why is PySpark SparkConf used?\n",
    "\n",
    "PySpark SparkConf is used for setting the configurations and parameters required to run applications on a cluster or local system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f27b6d",
   "metadata": {},
   "source": [
    "##  How to create SparkSession?\n",
    "\n",
    "To create SparkSession, we use the builder pattern. The SparkSession class from the pyspark.sql library has the getOrCreate() method which creates a new SparkSession if there is none or else it returns the existing SparkSession object. The following code is an example for creating SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58f72dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[1]\")\\\n",
    "                   .appName('InterviewBitSparkSession')\\\n",
    "                   .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd5676e",
   "metadata": {},
   "source": [
    "- master() – This is used for setting up the mode in which the application has to run - cluster mode (use the master name) or standalone mode. For Standalone mode, we use the local[x] value to the function, **where x represents partition count to be created in RDD, DataFrame and DataSet.** The value of **x is ideally the number of CPU cores available.**\n",
    "- appName() - Used for setting the application name\n",
    "- getOrCreate() – For returning SparkSession object. This creates a new object if it does not exist. If an object is there, it simply returns that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec173e8",
   "metadata": {},
   "source": [
    "## What are the different approaches for creating RDD in PySpark?\n",
    "\n",
    "The following image represents how we can visualize RDD creation in PySpark:\n",
    "\n",
    "<img width=\"773\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/195335900-af175180-0d5a-412f-b3e8-08a0d2a3d78d.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662aae36",
   "metadata": {},
   "source": [
    "In the image, we see that the data we have is the list form and post converting to RDDs, we have it stored in different partitions.\n",
    "We have the following approaches for creating PySpark RDD:\n",
    "\n",
    "- sc.parallelize(): The parallelize() method of the SparkContext can be used for creating RDDs. This method loads existing collection from the driver and parallelizes it. This is a basic approach to create RDD and is used when we have data already present in the memory. This also requires the presence of all data on the Driver before creating RDD. Code to create RDD using the parallelize method for the python list shown in the image above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07f0c74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[3] at readRDDFromFile at PythonRDD.scala:274\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]]\n"
     ]
    }
   ],
   "source": [
    "num_list = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=spark.sparkContext.parallelize(num_list)\n",
    "print(rdd)\n",
    "print(rdd.collect())\n",
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3dc8f8",
   "metadata": {},
   "source": [
    "- sc.textFile(): Using this method, we can read .txt file and convert them into RDD. Syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ca1f3102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IVP_Pyspark_1.ipynb        sample1.txt\r\n",
      "PySpark_Script_Template.py sample_1.txt\r\n",
      "Pyspark_2_practice_1.ipynb sample_2.txt\r\n",
      "Pyspark_practice_1.ipynb   test.txt\r\n",
      "pyspark_1.ipynb            test2.txt\r\n",
      "sample-text.txt            test5.txt\r\n",
      "sample.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56376f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my name is danny',\n",
       " 'your name is john',\n",
       " 'we are friend',\n",
       " 'good morning',\n",
       " 'the king',\n",
       " 'may the force be with you']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_txt = spark.sparkContext.textFile(\"./test2.txt\")\n",
    "rdd_txt.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ddd254",
   "metadata": {},
   "source": [
    "- sc.wholeTextFiles(): This function returns PairRDD (RDD containing key-value pairs) with **file path being the key and the file content is the value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f21891ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('file:/Users/sunghwanki/Desktop/Project/Python_Advance/Pyspark/test2.txt',\n",
       "  'my name is danny\\nyour name is john\\nwe are friend\\ngood morning\\nthe king\\nmay the force be with you\\n')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_whole_text = spark.sparkContext.wholeTextFiles(\"./test2.txt\")\n",
    "rdd_whole_text.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ab3749",
   "metadata": {},
   "source": [
    "We can also read csv, json, parquet and various other formats and create the RDDs.\n",
    "- Empty RDD with no partition using sparkContext.emptyRDD: RDD with no data is called empty RDD. We can create such RDDs having no partitions by using emptyRDD() method as shown in the code piece below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d320e2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method SparkContext.emptyRDD of <SparkContext master=local[1] appName=InterviewBitSparkSession>>\n"
     ]
    }
   ],
   "source": [
    "empty_rdd = spark.sparkContext.emptyRDD\n",
    "print(empty_rdd)\n",
    "\n",
    "# AttributeError: 'function' object has no attribute 'collect'\n",
    "# print(empty_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccff6c02",
   "metadata": {},
   "source": [
    "- Empty RDD with partitions using sparkContext.parallelize: When we do not require data but we require partition, then we create empty RDD by using the parallelize method as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7a51bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[11] at readRDDFromFile at PythonRDD.scala:274\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#Create empty RDD with 20 partitions\n",
    "empty_partitioned_rdd = spark.sparkContext.parallelize([],20) \n",
    "print(empty_partitioned_rdd)\n",
    "print(empty_partitioned_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff641a",
   "metadata": {},
   "source": [
    "## Is it possible to create PySpark DataFrame from external data sources?\n",
    "\n",
    "Yes, it is! Realtime applications make use of external file systems like local, HDFS, HBase, MySQL table, S3 Azure etc. Following example shows how we can create DataFrame by reading data from a csv file present in the local system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "98f8304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "header = ['name', 'area', 'country_code2', 'country_code3']\n",
    "data = [\n",
    "    ['Albania', 28748, 'AL', 'ALB'],\n",
    "    ['Algeria', 2381741, 'DZ', 'DZA'],\n",
    "    ['American Samoa', 199, 'AS', 'ASM'],\n",
    "    ['Andorra', 468, 'AD', 'AND'],\n",
    "    ['Angola', 1246700, 'AO', 'AGO']\n",
    "]\n",
    "\n",
    "with open('countries.csv', 'w', encoding='UTF8',  newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # write the header\n",
    "    writer.writerow(header)\n",
    "\n",
    "    # write the data (한줄씩 입력)\n",
    "    # writer.writerow(data)\n",
    "    \n",
    "    # write multiple rows\n",
    "    writer.writerows(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7d47c955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------------+-------------+\n",
      "|          name|   area|country_code2|country_code3|\n",
      "+--------------+-------+-------------+-------------+\n",
      "|       Albania|  28748|           AL|          ALB|\n",
      "|       Algeria|2381741|           DZ|          DZA|\n",
      "|American Samoa|    199|           AS|          ASM|\n",
      "|       Andorra|    468|           AD|          AND|\n",
      "|        Angola|1246700|           AO|          AGO|\n",
      "+--------------+-------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./countries.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "205aee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv header\n",
    "fieldnames = ['name', 'area', 'country_code2', 'country_code3']\n",
    "\n",
    "# csv data\n",
    "rows = [\n",
    "    {'name': 'Albania',\n",
    "    'area': 28748,\n",
    "    'country_code2': 'AL',\n",
    "    'country_code3': 'ALB'},\n",
    "    {'name': 'Algeria',\n",
    "    'area': 2381741,\n",
    "    'country_code2': 'DZ',\n",
    "    'country_code3': 'DZA'},\n",
    "    {'name': 'American Samoa',\n",
    "    'area': 199,\n",
    "    'country_code2': 'AS',\n",
    "    'country_code3': 'ASM'}\n",
    "]\n",
    "\n",
    "with open('d_countries.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d1908b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+-------------+-------------+\n",
      "|          name|   area|country_code2|country_code3|\n",
      "+--------------+-------+-------------+-------------+\n",
      "|       Albania|  28748|           AL|          ALB|\n",
      "|       Algeria|2381741|           DZ|          DZA|\n",
      "|American Samoa|    199|           AS|          ASM|\n",
      "+--------------+-------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"./d_countries.csv\", header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74bb118",
   "metadata": {},
   "source": [
    "## What do you understand by Pyspark’s startsWith() and endsWith() methods?\n",
    "\n",
    "These methods belong to the Column class and are used for searching DataFrame rows by checking if the column value starts with some value or ends with some value. They are used for filtering data in applications.\n",
    "\n",
    "- startsWith() – returns boolean Boolean value. It is true when the value of the column starts with the specified string and False when the match is not satisfied in that column value.\n",
    "\n",
    "- endsWith() – returns boolean Boolean value. It is true when the value of the column ends with the specified string and False when the match is not satisfied in that column value.\n",
    "\n",
    "Both the methods are case-sensitive.\n",
    "\n",
    "Consider an example of the startsWith() method here. We have created a DataFrame with 3 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6a98ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('Harry', 20),\n",
    "       ('Ron', 20),\n",
    "       ('Hermoine', 20)]\n",
    "columns = [\"Name\",\"Age\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd9504",
   "metadata": {},
   "source": [
    "If we have the below code that checks for returning the rows where all the names in the Name column start with “H”,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3a151e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|   Harry| 20|\n",
      "|Hermoine| 20|\n",
      "+--------+---+\n",
      "\n",
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|   Harry| 20|\n",
      "|Hermoine| 20|\n",
      "+--------+---+\n",
      "\n",
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "|   Harry|\n",
      "|Hermoine|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Name.startswith(\"H\")).show()\n",
    "df.filter(col(\"Name\").startswith(\"H\")).show()\n",
    "df.select(\"Name\").filter(df.Name.startswith(\"H\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5882437",
   "metadata": {},
   "source": [
    "Notice how the record with the Name “Ron” is filtered out because it does not start with “H”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586a140f",
   "metadata": {},
   "source": [
    "## What is PySpark SQL?\n",
    "\n",
    "PySpark SQL is the most popular PySpark module that is used to process structured columnar data. Once a DataFrame is created, we can interact with data using the SQL syntax. Spark SQL is used for bringing native raw SQL queries on Spark by using select, where, group by, join, union etc. For using PySpark SQL, the first step is to create a temporary table on DataFrame by using createOrReplaceTempView() function. Post creation, the table is accessible throughout SparkSession by using sql() method. When the SparkSession gets terminated, the temporary table will be dropped.\n",
    "For example, consider we have the following DataFrame assigned to a variable df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "214dc29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|   Harry| 20|\n",
      "|     Ron| 20|\n",
      "|Hermoine| 20|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c71cb47",
   "metadata": {},
   "source": [
    "In the below piece of code, we will be creating a temporary table of the DataFrame that gets accessible in the SparkSession using the sql() method. The SQL queries can be run within the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a874bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"STUDENTS\")\n",
    "df_new = spark.sql(\"SELECT * from STUDENTS\")\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fbfb025f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4141d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|    Name|Name_Count|\n",
      "+--------+----------+\n",
      "|Hermoine|         1|\n",
      "|   Harry|         1|\n",
      "|     Ron|         1|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupByGender = spark.sql(\"SELECT Name, count(*) as Name_Count from STUDENTS group by Name\")\n",
    "groupByGender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b9c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef0246b2",
   "metadata": {},
   "source": [
    "## Pyspark 함수 정리\n",
    "\n",
    "source : https://assaeunji.github.io/python/2022-03-26-pyspark/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b77e0e",
   "metadata": {},
   "source": [
    "### 전반적인 함수와 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af56d40",
   "metadata": {},
   "source": [
    "- printSchema() : 테이블의 스키마를 보여주는 함수\n",
    "- collect() : 테이블에서 행을 가져오는 함수\n",
    "- show(truncate=False) : 테이블 결과를 보여주는 함수, truncate = False를 사용하면 테이블 내용이 잘리지 않도록 보여줍니다.\n",
    "- describe() : 서머리 결과를 보여주는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31369f07",
   "metadata": {},
   "source": [
    "Pyspark의 함수는 대부분 SQL 언어와 비슷하게 구성되어 있습니다. SQL 언어와 비교하며 다음의 함수들에 대해 설명하겠습니다.\n",
    "\n",
    "<img width=\"908\" alt=\"image\" src=\"https://user-images.githubusercontent.com/26552500/195344599-1d82dfea-2e6c-4f7a-b286-5a6ee14d7ba8.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "562b6f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------+-----+------+\n",
      "|name                  |languages         |state|gender|\n",
      "+----------------------+------------------+-----+------+\n",
      "|{James, , Smith}      |[Java, Scala, C++]|OH   |M     |\n",
      "|{Anna, Rose, }        |[Spark, Java, C++]|NY   |F     |\n",
      "|{Julia, , Williams}   |[CSharp, VB]      |OH   |F     |\n",
      "|{Maria, Anne, Jones}  |[CSharp, VB]      |NY   |M     |\n",
      "|{Jen, Mary, Brown}    |[CSharp, VB]      |NY   |M     |\n",
      "|{Mike, Mary, Williams}|[Python, VB]      |OH   |M     |\n",
      "+----------------------+------------------+-----+------+\n",
      "\n",
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "data = [\n",
    "    ((\"James\",\"\",\"Smith\"),[\"Java\",\"Scala\",\"C++\"],\"OH\",\"M\"),\n",
    "    ((\"Anna\",\"Rose\",\"\"),[\"Spark\",\"Java\",\"C++\"],\"NY\",\"F\"),\n",
    "    ((\"Julia\",\"\",\"Williams\"),[\"CSharp\",\"VB\"],\"OH\",\"F\"),\n",
    "    ((\"Maria\",\"Anne\",\"Jones\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Jen\",\"Mary\",\"Brown\"),[\"CSharp\",\"VB\"],\"NY\",\"M\"),\n",
    "    ((\"Mike\",\"Mary\",\"Williams\"),[\"Python\",\"VB\"],\"OH\",\"M\")\n",
    " ]\n",
    "\n",
    "schema = StructType([\n",
    "     StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "         StructField('lastname', StringType(), True)\n",
    "     ])),\n",
    "     StructField('languages', ArrayType(StringType()), True),\n",
    "     StructField('state', StringType(), True),\n",
    "     StructField('gender', StringType(), True)\n",
    " ])\n",
    "\n",
    "df = spark.createDataFrame(data = data, schema = schema)\n",
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e86db3",
   "metadata": {},
   "source": [
    "### select와 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "81b4ed84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                name|\n",
      "+--------------------+\n",
      "|    {James, , Smith}|\n",
      "|      {Anna, Rose, }|\n",
      "| {Julia, , Williams}|\n",
      "|{Maria, Anne, Jones}|\n",
      "|  {Jen, Mary, Brown}|\n",
      "|{Mike, Mary, Will...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc19dd6",
   "metadata": {},
   "source": [
    "drop은 특정 컬럼만 제외하고 불러올 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "fa3b84ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------+\n",
      "|         languages|state|gender|\n",
      "+------------------+-----+------+\n",
      "|[Java, Scala, C++]|   OH|     M|\n",
      "|[Spark, Java, C++]|   NY|     F|\n",
      "|      [CSharp, VB]|   OH|     F|\n",
      "|      [CSharp, VB]|   NY|     M|\n",
      "|      [CSharp, VB]|   NY|     M|\n",
      "|      [Python, VB]|   OH|     M|\n",
      "+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(\"name\").show() ## name을 제외한 컬럼을 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4074c23",
   "metadata": {},
   "source": [
    "### count와 countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6ef4e039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------+\n",
      "|count(state)|count(DISTINCT state)|\n",
      "+------------+---------------------+\n",
      "|           6|                    2|\n",
      "+------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(count(\"state\"), countDistinct(\"state\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e63ac8",
   "metadata": {},
   "source": [
    "### withColumn과 withColumnRenamed\n",
    "\n",
    "withColumn은 컬럼의 정보를 바꾸고자 할 때 혹은 새로운 컬럼을 추가할 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "157c0e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                Name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed(\"변경 전\", \"변경 후\")\n",
    "df.withColumnRenamed(\"name\", \"Name\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a376258a",
   "metadata": {},
   "source": [
    "### withColumn으로 컬럼 타입 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4e927919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"state\", df.state.cast(\"String\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "440674d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: struct (nullable = true)\n",
      " |    |-- firstname: string (nullable = true)\n",
      " |    |-- middlename: string (nullable = true)\n",
      " |    |-- lastname: string (nullable = true)\n",
      " |-- languages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d19d81",
   "metadata": {},
   "source": [
    "### withColumn으로 컬럼 추가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b0c072d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+-------+\n",
      "|                name|         languages|state|gender|Country|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|     M|  U.S.A|\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|     F|  U.S.A|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|     F|  U.S.A|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|     M|  U.S.A|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|     M|  U.S.A|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|     M|  U.S.A|\n",
      "+--------------------+------------------+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"Country\", lit(\"U.S.A\")).show() \n",
    "# lit: 문자열(literal value)로 새로운 컬럼을 만들 때 사용하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed2fb52",
   "metadata": {},
   "source": [
    "### withColumn으로 CASE WHEN 구문 쓰기\n",
    "\n",
    "when과 otherwise 함수를 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "311f5c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+-----+------+\n",
      "|                name|         languages|state|gender|\n",
      "+--------------------+------------------+-----+------+\n",
      "|    {James, , Smith}|[Java, Scala, C++]|   OH|  Male|\n",
      "|      {Anna, Rose, }|[Spark, Java, C++]|   NY|Female|\n",
      "| {Julia, , Williams}|      [CSharp, VB]|   OH|Female|\n",
      "|{Maria, Anne, Jones}|      [CSharp, VB]|   NY|  Male|\n",
      "|  {Jen, Mary, Brown}|      [CSharp, VB]|   NY|  Male|\n",
      "|{Mike, Mary, Will...|      [Python, VB]|   OH|  Male|\n",
      "+--------------------+------------------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"gender\"\n",
    "                    , when(df.gender == \"F\", \"Female\")\n",
    "                   .when(df.gender==\"M\", \"Male\")\n",
    "                   .when(df.gender.isNull(), \"\")\n",
    "                   .otherwise(df.gender)\n",
    "                   ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c52db0f",
   "metadata": {},
   "source": [
    "이를 SQL로 쓰면 다음과 같습니다.\n",
    "\n",
    "```\n",
    "SELECT *\n",
    "    , CASE WHEN gender = 'F' THEN 'Female'\n",
    "        WHEN gender = 'M'    THEN 'Male'\n",
    "        WHEN gender IS NULL  THEN ''\n",
    "        ELSE gender \n",
    "    END as gender2\n",
    "FROM df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13f556",
   "metadata": {},
   "source": [
    "### flatMap VS Map\n",
    "\n",
    "map과 비슷하지만 map에서 이중 리스트로 표현되는 데이터를 하나의 리스트에서 전부 나열해줍니다.\n",
    "\n",
    "아래 4개의 문자열이 있는 리스트를 RDD로 생성하고 동일한 lambda 함수를 인자로 각각 map과 flatMap을 사용합니다.\n",
    "\n",
    "map을 사용한 RDD는 lambda 함수로 인해 문자열과 문자열 끝에 s가 붙은 문자열의 쌍으로 이루어져 있지만 flatMap을 사용한 RDD는 모든 문자열이 하나의 리스트안에 나열되어있습니다.\n",
    "\n",
    "각 RDD를 count함수를 사용하면 결과가 다르게 나오는 것을 확인할 수 있습니다.\n",
    "\n",
    "이렇게 모든 데이터를 하나의 리스트로 변환해주기 때문에 워드카운트와 같은 작업에 flatMap이 유용하게 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09a285cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'elephant', 'tiger']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animals = ['cat','dog','elephant', 'tiger']\n",
    "animalsRdd = sc.parallelize(animals)\n",
    "animalsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71ead82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('cat', 'cats'), ('dog', 'dogs'), ('elephant', 'elephants'), ('tiger', 'tigers')]\n",
      "['cat', 'cats', 'dog', 'dogs', 'elephant', 'elephants', 'tiger', 'tigers']\n"
     ]
    }
   ],
   "source": [
    "animalsRDDMap = animalsRdd.map(lambda x: (x, x+'s'))\n",
    "animalsRDDFlatmap = animalsRdd.flatMap(lambda x: (x,x+'s'))\n",
    "\n",
    "print(animalsRDDMap.collect())\n",
    "print(animalsRDDFlatmap.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfb437ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(animalsRDDMap.count())\n",
    "print(animalsRDDFlatmap.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd44ac01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cat', 'cats'),\n",
       " ('dog', 'dogs'),\n",
       " ('elephant', 'elephants'),\n",
       " ('tiger', 'tigers')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def animals(data):\n",
    "    return data, data+'s'\n",
    "\n",
    "animalsRdd.map(animals).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6f73879",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'cats', 'dog', 'dogs', 'elephant', 'elephants', 'tiger', 'tigers']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animalsRdd.flatMap(animals).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0288fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Gutenberg’s\n",
      "Alice’s Adventures in Wonderland\n",
      "Project Gutenberg’s\n",
      "Adventures in Wonderland\n",
      "Project Gutenberg’s\n"
     ]
    }
   ],
   "source": [
    "data = [\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"]\n",
    "\n",
    "rdd = sc.parallelize(data)\n",
    "for i in rdd.collect():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72bd7a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg’s',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'Project Gutenberg’s',\n",
       " 'Adventures in Wonderland',\n",
       " 'Project Gutenberg’s']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50384df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sub2(n):\n",
    "    return n.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "baffdbbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project',\n",
       " 'Gutenberg’s',\n",
       " 'Alice’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenberg’s',\n",
       " 'Adventures',\n",
       " 'in',\n",
       " 'Wonderland',\n",
       " 'Project',\n",
       " 'Gutenberg’s']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.flatMap(sub2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78903754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Project', 'Gutenberg’s'],\n",
       " ['Alice’s', 'Adventures', 'in', 'Wonderland'],\n",
       " ['Project', 'Gutenberg’s'],\n",
       " ['Adventures', 'in', 'Wonderland'],\n",
       " ['Project', 'Gutenberg’s']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(sub2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db042d74",
   "metadata": {},
   "source": [
    "### Using flatMap() transformation on DataFrame\n",
    "\n",
    "Unfortunately, PySpark DataFame doesn’t have flatMap() transformation however, DataFrame has explode() SQL function that is used to flatten the column. Below is a complete example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "19a0f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "arrayData = [\n",
    "        ('James',['Java','Scala'],{'hair':'black','eye':'brown'}),\n",
    "        ('Michael',['Spark','Java',None],{'hair':'brown','eye':None}),\n",
    "        ('Robert',['CSharp',''],{'hair':'red','eye':''}),\n",
    "        ('Washington',None,None),\n",
    "        ('Jefferson',['1','2'],{})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7a65d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: array<string>, _3: map<string,string>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(arrayData)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfc1bb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+\n",
      "|        _1|                 _2|                  _3|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbf87394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+--------------------+\n",
      "|      name|     knownLanguages|          properties|\n",
      "+----------+-------------------+--------------------+\n",
      "|     James|      [Java, Scala]|{eye -> brown, ha...|\n",
      "|   Michael|[Spark, Java, null]|{eye -> null, hai...|\n",
      "|    Robert|         [CSharp, ]|{eye -> , hair ->...|\n",
      "|Washington|               null|                null|\n",
      "| Jefferson|             [1, 2]|                  {}|\n",
      "+----------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(arrayData, schema=['name', 'knownLanguages', 'properties'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15656d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- knownLanguages: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- properties: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53d70b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|     name|   col|\n",
      "+---------+------+\n",
      "|    James|  Java|\n",
      "|    James| Scala|\n",
      "|  Michael| Spark|\n",
      "|  Michael|  Java|\n",
      "|  Michael|  null|\n",
      "|   Robert|CSharp|\n",
      "|   Robert|      |\n",
      "|Jefferson|     1|\n",
      "|Jefferson|     2|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# df2 = df.select('name')\n",
    "df2 = df.select(df.name, explode(df.knownLanguages))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c49074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- col: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5795a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "#spark.sparkContext.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a1a739",
   "metadata": {},
   "source": [
    "### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "56995fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['interview', 'interviewbit']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"Transdormation Demo\")\n",
    "words_list = sc.parallelize (\n",
    "  [\"pyspark\", \n",
    "  \"interview\", \n",
    "  \"questions\", \n",
    "  \"at\", \n",
    "  \"interviewbit\"]\n",
    ")\n",
    "filtered_words = words_list.filter(lambda x: 'interview' in x)\n",
    "filtered = filtered_words.collect()\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da0f444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of elements in RDD ->  2\n"
     ]
    }
   ],
   "source": [
    "counts = filtered_words.count()\n",
    "print(\"Count of elements in RDD -> \",  counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017c9aec",
   "metadata": {},
   "source": [
    "### Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad084357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./test2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./test2.txt\n",
    "my name is danny\n",
    "your name is john\n",
    "we are friend\n",
    "good morning\n",
    "the king\n",
    "may the force be with you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "deca1e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 2),\n",
       " ('is', 2),\n",
       " ('danny', 1),\n",
       " ('john', 1),\n",
       " ('we', 1),\n",
       " ('are', 1),\n",
       " ('good', 1),\n",
       " ('king', 1),\n",
       " ('may', 1),\n",
       " ('my', 1),\n",
       " ('your', 1),\n",
       " ('friend', 1),\n",
       " ('morning', 1),\n",
       " ('the', 2),\n",
       " ('force', 1),\n",
       " ('be', 1),\n",
       " ('with', 1),\n",
       " ('you', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "myRdd = sc.textFile(os.path.abspath('./test2.txt'))\n",
    "myRdd = myRdd.flatMap(lambda x: x.split(\" \")).map(lambda word:(word,1)).reduceByKey(lambda x,y: x+y)\n",
    "myRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fa3e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: 2\n",
      "is: 2\n",
      "danny: 1\n",
      "john: 1\n",
      "we: 1\n",
      "are: 1\n",
      "good: 1\n",
      "king: 1\n",
      "may: 1\n",
      "my: 1\n",
      "your: 1\n",
      "friend: 1\n",
      "morning: 1\n",
      "the: 2\n",
      "force: 1\n",
      "be: 1\n",
      "with: 1\n",
      "you: 1\n"
     ]
    }
   ],
   "source": [
    "output = myRdd.collect()\n",
    "for (word,count) in output:\n",
    "    print(\"%s: %i\"% (word, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad6252",
   "metadata": {},
   "source": [
    "### 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "016f9015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",\"Sales\",\"NY\",90000,34,10000), \\\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000), \\\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000), \\\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000), \\\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000), \\\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000), \\\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000), \\\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000), \\\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000) \\\n",
    "  ]\n",
    "columns = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]\n",
    "df = spark.createDataFrame(simpleData, schema = columns )\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3cd8a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"department\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92dac667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(\"department\", \"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62634ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.sort(col(\"department\"), col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9652e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"department\"), col(\"salary\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46ea6f33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"EMP\")\n",
    "spark.sql(\"select * from EMP order by department, salary desc\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da5380d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is danny\n",
      "['my name is danny', 'your name is john', 'we are friend']\n",
      "['my name is danny', 'your name is john', 'we are friend', 'good morning', 'the king', 'may the force be with you']\n"
     ]
    }
   ],
   "source": [
    "tempdir='/Users/sunghwanki/Desktop/Project/Python_Advance/Pyspark'\n",
    "myRdd1 = sc.textFile(os.path.join(tempdir, 'test2.txt'))\n",
    "print(myRdd1.first())\n",
    "print(myRdd1.take(3))\n",
    "print(myRdd1.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59d5c510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "dataRange = [i for i in range(20)]\n",
    "print(dataRange)\n",
    "print(type(dataRange))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99d8348b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[61] at readRDDFromFile at PythonRDD.scala:274\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "rangeRdd = spark.sparkContext.parallelize(dataRange)\n",
    "print(rangeRdd)\n",
    "print(type(rangeRdd))\n",
    "print(rangeRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b42084c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "def sub(n):\n",
    "    return n-1\n",
    "\n",
    "subRdd = rangeRdd.map(sub)\n",
    "print(subRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77233577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
